{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26f985ff-1242-4bb7-922d-0b02a8d007b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/Text Mining/StemLem\n"
     ]
    }
   ],
   "source": [
    "cd ./StemLem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68b14617-bdb5-4e95-9399-37b4a3081174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sing\n",
      "singer\n",
      "singer\n",
      "sing\n",
      "song\n",
      "analysi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###########################################\n",
    "##\n",
    "##\n",
    "##  READ ME FIRST\n",
    "##\n",
    "##\n",
    "## This code does not \"run linearly\"\n",
    "## Instead, it is a collection of different options\n",
    "## that can be used together or not and in many ways.\n",
    "##\n",
    "## To use this code, review and understand it first.\n",
    "## Then comment or uncomment (and adjust) what you wish\n",
    "## \n",
    "## You will also need to CREATE YOUR OWN DATA to use\n",
    "## this code.\n",
    "############################################################\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "LEMMER = WordNetLemmatizer() \n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "STEMMER=PorterStemmer()\n",
    "\n",
    "\n",
    "print(STEMMER.stem(\"singing\"))\n",
    "print(LEMMER.lemmatize(\"singers\"))\n",
    "\n",
    "print(STEMMER.stem(\"singers\"))\n",
    "print(STEMMER.stem(\"sings\"))\n",
    "print(STEMMER.stem(\"songs\"))\n",
    "print(STEMMER.stem(\"analysis\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d02442f7-aef6-4e2f-a950-a9799621132f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hike', 'is', 'love', 'by', 'hiker', 'and', 'hike']\n",
      "['hiking', 'is', 'loved', 'by', 'hiker', 'and', 'hike']\n"
     ]
    }
   ],
   "source": [
    "# Use NLTK's PorterStemmer in a function\n",
    "def MY_STEMMER(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z\\-]\", \" \", str_input).lower().split()\n",
    "    words = [STEMMER.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "def MY_LEMMER(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z\\-]\", \" \", str_input).lower().split()\n",
    "    words = [LEMMER.lemmatize(word) for word in words]\n",
    "    return words\n",
    "\n",
    "print(MY_STEMMER((\"Hiking is loved by hikers and hikes!! 1234\")))\n",
    "print(MY_LEMMER(\"Hiking is loved by hikers and hikes!! 1234\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "86bdd004-e7f7-4c5f-b6a5-b6faec71bcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1 = pd.read_csv('Text1_clean.csv', usecols = [1])\n",
    "df2 = pd.read_csv('Text2_clean.csv', usecols = [1])\n",
    "df3 = pd.read_csv('Text3_clean.csv', usecols = [1])\n",
    "df4 = pd.read_csv('Text4_clean.csv', usecols = [1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "66b5a1b2-8cf0-40ee-8fa4-af3d1cefac69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/Text Mining/StemLem/CleanTextFiles\n"
     ]
    }
   ],
   "source": [
    "cd ./CleanTextFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9f48beee-5436-43b5-8ea4-462c2aa6f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_list = list(df1.iloc[:,0]) \n",
    "  \n",
    "# converting list into string and then joining it with space \n",
    "text1 = ' '.join(str(e) for e in df1_list) \n",
    "  \n",
    "# printing result \n",
    "#print(text1)\n",
    "\n",
    "text_file1 = open(r'Text1.txt', 'w')\n",
    "text_file1.write(text1)\n",
    "text_file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "212bf13c-a65a-4613-a68a-4054e5817995",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_list = list(df2.iloc[:,0]) \n",
    "  \n",
    "# converting list into string and then joining it with space \n",
    "text2 = ' '.join(str(e) for e in df2_list) \n",
    "  \n",
    "# printing result \n",
    "#print(text1)\n",
    "\n",
    "text_file2 = open(r'Text2.txt', 'w')\n",
    "text_file2.write(text2)\n",
    "text_file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "75b23777-dad0-4097-9b31-00f6b959cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_list = list(df3.iloc[:,0]) \n",
    "  \n",
    "# converting list into string and then joining it with space \n",
    "text3 = ' '.join(str(e) for e in df3_list) \n",
    "  \n",
    "# printing result \n",
    "#print(text1)\n",
    "\n",
    "text_file3 = open(r'Text3.txt', 'w')\n",
    "text_file3.write(text3)\n",
    "text_file3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bc0fb7e6-d73c-418a-9506-1cf0d7dc60e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_list = list(df4.iloc[:,0]) \n",
    "  \n",
    "# converting list into string and then joining it with space \n",
    "text4 = ' '.join(str(e) for e in df4_list) \n",
    "  \n",
    "# printing result \n",
    "#print(text1)\n",
    "\n",
    "text_file4 = open(r'Text4.txt', 'w')\n",
    "text_file4.write(text4)\n",
    "text_file4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "28c2c83a-ee72-46c5-9c1b-c699f28b28c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/Text Mining/StemLem/CleanTextFiles'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8871484a-3ab1-4f96-a712-dd77f0d271b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text2.txt', 'Text4.txt', 'Text3.txt', 'Text1.txt']\n",
      "full list...\n",
      "['/home/jovyan/Text Mining/StemLem/CleanTextFiles/Text2.txt', '/home/jovyan/Text Mining/StemLem/CleanTextFiles/Text4.txt', '/home/jovyan/Text Mining/StemLem/CleanTextFiles/Text3.txt', '/home/jovyan/Text Mining/StemLem/CleanTextFiles/Text1.txt']\n"
     ]
    }
   ],
   "source": [
    "all_file_names = []\n",
    "\n",
    "## Put YOUR path here and create your own corpus. Mine here is\n",
    "## called Dog_Hike. My Dog_Hike corpus contains 20 .txt documents\n",
    "## some about dogs and some about hiking and each a different length.\n",
    "path=\"/home/jovyan/Text Mining/StemLem/CleanTextFiles/\"\n",
    "\n",
    "#shutil.rmtree('/home/jovyan/Text Mining/StemLem/CleanTextFiles/.ipynb_checkpoints')\n",
    "\n",
    "\n",
    "#print(\"calling os...\")\n",
    "#print(os.listdir(path))\n",
    "FileNameList=os.listdir(path)\n",
    "print(FileNameList)\n",
    "\n",
    "ListOfCompleteFiles=[]\n",
    "\n",
    "for name in os.listdir(path):\n",
    "    #print(path+ \"/\" + name)\n",
    "    next1=path+ \"\" + name\n",
    "    ListOfCompleteFiles.append(next1)\n",
    "#print(\"DONE...\")\n",
    "print(\"full list...\")\n",
    "print(ListOfCompleteFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cc9998e7-98fc-4b4a-a4a8-5b0440bcd167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1121\n",
      "   able  academy  accounts  accused  accusing  action  activist  activists  \\\n",
      "0     2        0         0        0         0       0         1          1   \n",
      "1     0        1         1        0         1       0         0          0   \n",
      "2     0        1         0        1         0       1         0          0   \n",
      "3     0        0         0        0         0       0         0          0   \n",
      "\n",
      "   actor  actors  ...  year  years  yes  yesterday  york  young  youtube  \\\n",
      "0      4       0  ...     1      0    0          0     2      3        0   \n",
      "1      0       0  ...     1      1    0          1     0      0        4   \n",
      "2      0       1  ...     1      1    0          0     0      0        0   \n",
      "3      0       0  ...     2      1    1          0     1      1        0   \n",
      "\n",
      "   zachary  zealand  zuckerberg  \n",
      "0        1        0           0  \n",
      "1        0        0           1  \n",
      "2        0        1           0  \n",
      "3        0        0           1  \n",
      "\n",
      "[4 rows x 1121 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### For CountVectorizer Options----------------\n",
    "MyVect1=CountVectorizer(input='filename', stop_words=\"english\")\n",
    "#MyVect1=CountVectorizer(input='filename', stop_words=\"english\", max_df=4, min_df=2)\n",
    "#MyVect1=CountVectorizer(input='filename', stop_words=\"english\", max_features=12)\n",
    "##path=\"C:\\\\Users\\\\profa\\\\Documents\\\\Python Scripts\\\\TextMining\\\\DATA\\\\SmallTextDocs\"\n",
    "Vect_DH = MyVect1.fit_transform(ListOfCompleteFiles)\n",
    "CV_Stopwords=MyVect1.get_stop_words()\n",
    "ColumnNames1=MyVect1.get_feature_names_out()\n",
    "print(len(ColumnNames1))\n",
    "CorpusDF_DH=pd.DataFrame(Vect_DH.toarray(),columns=ColumnNames1)\n",
    "print(CorpusDF_DH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "94b97b90-da31-44eb-9113-b5056de456ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ohio</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affirming</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>republican</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actor</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>american</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senate</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>january</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>care</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proposals</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>announced</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>net</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>young</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dozens</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talented</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elmo</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tove</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0  1  2   3\n",
       "ohio        8  0  0  13\n",
       "new         5  8  4   3\n",
       "affirming   4  0  1   2\n",
       "gender      4  0  0   5\n",
       "republican  4  0  0   7\n",
       "actor       4  0  0   0\n",
       "state       4  0  2   7\n",
       "american    4  1  1   0\n",
       "world       4  2  0   3\n",
       "senate      3  0  0   5\n",
       "january     3  0  0   1\n",
       "care        3  0  0   2\n",
       "proposals   3  0  0   1\n",
       "announced   3  1  3   2\n",
       "net         3  0  0   0\n",
       "young       3  0  0   1\n",
       "dozens      2  0  0   0\n",
       "talented    2  0  0   0\n",
       "elmo        2  0  0   0\n",
       "tove        2  0  0   0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CorpusDF_DH.T.sort_values(by=0, ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f09ff3d3-0f8f-4f2a-a2bb-a699ef994217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'cannot', 'either', 'go', 'hasnt', 'even', 'however', 'describe', 'was', 'whenever', 'under', 'yourselves', 'from', 'somehow', 'top', 'on', 'why', 'whose', 'herein', 'has', 'etc', 'to', 'their', 'him', 'ever', 'themselves', 'which', 'per', 'thru', 'for', 'neither', 'next', 'me', 'by', 'once', 'else', 'ours', 'into', 'mill', 'twelve', 'besides', 'though', 'elsewhere', 'hereafter', 'below', 'between', 'his', 'herself', 'during', 'thereby', 'being', 'indeed', 'mostly', 'have', 'himself', 'too', 'two', 'formerly', 'may', 'none', 'whereas', 'everything', 'or', 'because', 'inc', 'those', 'throughout', 'cry', 'any', 'amongst', 'third', 'twenty', 'bill', 'while', 'can', 'although', 'much', 'somewhere', 'becomes', 'itself', 'i', 'am', 'empty', 'but', 'nothing', 'never', 'former', 'sincere', 'others', 'detail', 'therefore', 'whereby', 'beforehand', 'forty', 'often', 'serious', 'nevertheless', 'interest', 'across', 'find', 'with', 'mine', 'nine', 'amoungst', 'do', 'back', 'at', 'are', 'our', 'thereupon', 'whereupon', 'something', 're', 'name', 'afterwards', 'within', 'thick', 'seem', 'above', 'de', 'whether', 'ltd', 'latterly', 'seemed', 'hers', 'about', 'put', 'noone', 'further', 'give', 'hereby', 'still', 'already', 'her', 'seeming', 'might', 'thereafter', 'after', 'own', 'otherwise', 'ten', 'latter', 'so', 'there', 'behind', 'be', 'before', 'beyond', 'couldnt', 'against', 'enough', 'both', 'part', 'along', 'they', 'whence', 'been', 'everywhere', 'amount', 'few', 'please', 'via', 'became', 'seems', 'ie', 'a', 'several', 'who', 'first', 'upon', 'move', 'us', 'fifteen', 'thin', 'anything', 'towards', 'among', 'that', 'you', 'other', 'where', 'of', 'each', 'con', 'my', 'least', 'get', 'up', 'anyhow', 'always', 'this', 'done', 'five', 'were', 'had', 'everyone', 'eleven', 'will', 'thus', 'almost', 'only', 'ourselves', 'over', 'she', 'system', 'eg', 'through', 'again', 'someone', 'than', 'out', 'whereafter', 'fifty', 'sixty', 'the', 'these', 'one', 'cant', 'hence', 'would', 'could', 'anywhere', 'becoming', 'an', 'then', 'all', 'nor', 'well', 'eight', 'whoever', 'we', 'off', 'wherein', 'six', 'rather', 'down', 'them', 'front', 'is', 'therein', 'must', 'some', 'whither', 'four', 'no', 'co', 'whatever', 'bottom', 'meanwhile', 'onto', 'if', 'as', 'nobody', 'when', 'three', 'how', 'it', 'alone', 'yet', 'since', 'yourself', 'last', 'such', 'hundred', 'without', 'due', 'here', 'until', 'and', 'toward', 'thence', 'less', 'hereupon', 'also', 'show', 'keep', 'not', 'sometime', 'fire', 'in', 'together', 'should', 'nowhere', 'moreover', 'myself', 'every', 'same', 'namely', 'more', 'whole', 'most', 'take', 'yours', 'call', 'beside', 'very', 'fill', 'un', 'made', 'except', 'he', 'whom', 'wherever', 'full', 'anyway', 'many', 'perhaps', 'another', 'anyone', 'see', 'what', 'side', 'around', 'now', 'its', 'sometimes', 'your', 'become', 'found'})\n",
      "267\n",
      "dr\n",
      "ld\n",
      "pr\n",
      "se\n",
      "secureoursocials\n",
      "th\n",
      "   able  academy  accounts  accused  accusing  action  activist  activists  \\\n",
      "0     2        0         0        0         0       0         1          1   \n",
      "1     0        1         1        0         1       0         0          0   \n",
      "2     0        1         0        1         0       1         0          0   \n",
      "3     0        0         0        0         0       0         0          0   \n",
      "\n",
      "   actor  actors  ...  year  years  yes  yesterday  york  young  youtube  \\\n",
      "0      4       0  ...     1      0    0          0     2      3        0   \n",
      "1      0       0  ...     1      1    0          1     0      0        4   \n",
      "2      0       1  ...     1      1    0          0     0      0        0   \n",
      "3      0       0  ...     2      1    1          0     1      1        0   \n",
      "\n",
      "   zachary  zealand  zuckerberg  \n",
      "0        1        0           0  \n",
      "1        0        0           1  \n",
      "2        0        1           0  \n",
      "3        0        0           1  \n",
      "\n",
      "[4 rows x 1115 columns]\n",
      "0    451\n",
      "1    411\n",
      "2    405\n",
      "3    432\n",
      "dtype: int64\n",
      "       able   academy  accounts   accused  accusing    action  activist  \\\n",
      "0  0.004435  0.000000  0.000000  0.000000  0.000000  0.000000  0.002217   \n",
      "1  0.000000  0.002433  0.002433  0.000000  0.002433  0.000000  0.000000   \n",
      "2  0.000000  0.002469  0.000000  0.002469  0.000000  0.002469  0.000000   \n",
      "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "   activists     actor    actors  ...      year     years       yes  \\\n",
      "0   0.002217  0.008869  0.000000  ...  0.002217  0.000000  0.000000   \n",
      "1   0.000000  0.000000  0.000000  ...  0.002433  0.002433  0.000000   \n",
      "2   0.000000  0.000000  0.002469  ...  0.002469  0.002469  0.000000   \n",
      "3   0.000000  0.000000  0.000000  ...  0.004630  0.002315  0.002315   \n",
      "\n",
      "   yesterday      york     young   youtube   zachary   zealand  zuckerberg  \n",
      "0   0.000000  0.004435  0.006652  0.000000  0.002217  0.000000    0.000000  \n",
      "1   0.002433  0.000000  0.000000  0.009732  0.000000  0.000000    0.002433  \n",
      "2   0.000000  0.000000  0.000000  0.000000  0.000000  0.002469    0.000000  \n",
      "3   0.000000  0.002315  0.002315  0.000000  0.000000  0.000000    0.002315  \n",
      "\n",
      "[4 rows x 1115 columns]\n"
     ]
    }
   ],
   "source": [
    "## Aggregate by hand\n",
    "#CorpusDF_DH[\"dog\"]= CorpusDF_DH[\"dog\"]+ CorpusDF_DH[\"dogs\"]\n",
    "#CorpusDF_DH= CorpusDF_DH.drop([\"dogs\"], axis=1)\n",
    "\n",
    "\n",
    "print(CV_Stopwords)\n",
    "print(list(CV_Stopwords).index(\"and\"))\n",
    "\n",
    "#RemoveWords=[\"plan\", \"great\"]\n",
    "RemoveWords=[\"and\"]\n",
    "\n",
    "for nextcol in CorpusDF_DH.columns:\n",
    "    if(re.search(r'[^A-Za-z]+', nextcol)):\n",
    "        #print(nextcol)\n",
    "         CorpusDF_DH= CorpusDF_DH.drop([nextcol], axis=1)\n",
    "#    ## The following will remove any column with name\n",
    "#    ## of 3 or smaller - like \"it\" or \"of\" or \"pre\".\n",
    "#    ##print(len(nextcol))  ## check it first\n",
    "#    ## NOTE: You can also use this code to CONTROL\n",
    "#    ## the words in the columns. For example - you can\n",
    "#    ## have only words between lengths 5 and 9. \n",
    "#    ## In this case, we remove columns with words <= 3.\n",
    "    elif(len(str(nextcol))<3):\n",
    "        print(nextcol)\n",
    "        CorpusDF_DH= CorpusDF_DH.drop([nextcol], axis=1)\n",
    "    elif(len(str(nextcol))>15):\n",
    "        print(nextcol)\n",
    "        CorpusDF_DH= CorpusDF_DH.drop([nextcol], axis=1)\n",
    "    elif(nextcol in RemoveWords):\n",
    "        print(nextcol)\n",
    "        CorpusDF_DH= CorpusDF_DH.drop([nextcol], axis=1)\n",
    "        \n",
    "\n",
    "print(CorpusDF_DH)\n",
    "TheSums=CorpusDF_DH.sum(axis=1)\n",
    "print(TheSums)\n",
    "\n",
    "\n",
    "## Include a SUM column...\n",
    "#CorpusDF_DH[\"SUM\"] = CorpusDF_DH.agg(\"sum\",axis=1)\n",
    "## Alternative method...\n",
    "#df['C'] = df.sum(axis=1)\n",
    "\n",
    "## Divide each row by its SUM- normalize - \n",
    "NormDF = CorpusDF_DH.div(TheSums, axis=0)\n",
    "print(NormDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124521b8-a5b2-4cb8-8d9a-6f6c017ffb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c455a-3d5e-42a3-979d-e08e88f0aff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a8cc9c7e-5c07-471a-9af4-49f987f3be04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   announc  ban  gender  gov  ha  new  ohio  republican  state  thi\n",
      "0        4    4       4    2  10    5     8           5      5    2\n",
      "1        3    4       0    1  14    8     0           0      0    2\n",
      "2        3    0       0    0   7    4     0           0      2    3\n",
      "3        2    5       5    9   7    3    13           9      9    3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "##---------------------\n",
    "## Using Stemming and Lemming\n",
    "##-------------------------------------\n",
    "MyVect_STEM=CountVectorizer(input='filename',\n",
    "                        analyzer = 'word',\n",
    "                        stop_words='english',\n",
    "                        tokenizer=MY_STEMMER,\n",
    "                        lowercase = True,\n",
    "                        max_features=10\n",
    "                        )\n",
    "\n",
    "Vect_Stem = MyVect_STEM.fit_transform(ListOfCompleteFiles)\n",
    "ColumnNames_s=MyVect_STEM.get_feature_names_out()\n",
    "CorpusDF_Stem=pd.DataFrame(Vect_Stem.toarray(),columns=ColumnNames_s)\n",
    "print(CorpusDF_Stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "348063a5-6ca6-477b-b3d8-c4a5c502022f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   american  announced  ban  gender  gov  ha  new  ohio  republican  state\n",
      "0         5          3    3       4    2  10    5     8           5      5\n",
      "1         1          1    2       0    1  14    8     0           0      0\n",
      "2         1          3    0       0    0   7    4     0           0      2\n",
      "3         2          2    5       5    9   7    3    13           9      9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MyVect_LEM=CountVectorizer(input='filename',\n",
    "                        analyzer = 'word',\n",
    "                        stop_words='english',\n",
    "                        tokenizer=MY_LEMMER,\n",
    "                        lowercase = True,\n",
    "                        max_features=10\n",
    "                        )\n",
    "\n",
    "\n",
    "Vect_LEM = MyVect_LEM.fit_transform(ListOfCompleteFiles)\n",
    "ColumnNames_lem=MyVect_LEM.get_feature_names_out()\n",
    "CorpusDF_LEM=pd.DataFrame(Vect_LEM.toarray(),columns=ColumnNames_lem)\n",
    "print(CorpusDF_LEM)\n",
    "#Search &help;\n",
    "#Â© 2024 Gates Bolton Analytics | Theme by Theme Ansar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fe3db9f9-9a91-4b1a-858a-b74e1ce83b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       able   academy  accounts   accused  accusing    action  activist  \\\n",
      "0  0.086099  0.000000   0.00000  0.000000   0.00000  0.000000  0.043049   \n",
      "1  0.000000  0.036677   0.04652  0.000000   0.04652  0.000000  0.000000   \n",
      "2  0.000000  0.037222   0.00000  0.047211   0.00000  0.047211  0.000000   \n",
      "3  0.000000  0.000000   0.00000  0.000000   0.00000  0.000000  0.000000   \n",
      "\n",
      "   activists     actor    actors  ...      year     years       yes  \\\n",
      "0   0.043049  0.172197  0.000000  ...  0.022465  0.000000  0.000000   \n",
      "1   0.000000  0.000000  0.000000  ...  0.024276  0.029693  0.000000   \n",
      "2   0.000000  0.000000  0.047211  ...  0.024637  0.030134  0.000000   \n",
      "3   0.000000  0.000000  0.000000  ...  0.040842  0.024978  0.039133   \n",
      "\n",
      "   yesterday      york     young   youtube   zachary   zealand  zuckerberg  \n",
      "0    0.00000  0.067881  0.101822  0.000000  0.043049  0.000000    0.000000  \n",
      "1    0.04652  0.000000  0.000000  0.186081  0.000000  0.000000    0.036677  \n",
      "2    0.00000  0.000000  0.000000  0.000000  0.000000  0.047211    0.000000  \n",
      "3    0.00000  0.030853  0.030853  0.000000  0.000000  0.000000    0.030853  \n",
      "\n",
      "[4 rows x 1121 columns]\n"
     ]
    }
   ],
   "source": [
    "MyVect_TF=TfidfVectorizer(input='filename', stop_words=\"english\")\n",
    "Vect = MyVect_TF.fit_transform(ListOfCompleteFiles)\n",
    "ColumnNamesTF=MyVect_TF.get_feature_names_out()\n",
    "CorpusDF_TF=pd.DataFrame(Vect.toarray(),columns=ColumnNamesTF)\n",
    "print(CorpusDF_TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e28b5-ca72-4df7-b611-78f43640cdd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6493da6-cdd4-4a03-9801-5e64712db461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5605202-04ef-4ee7-890b-4933ce9b7847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
